<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-11-15 Wed 17:23 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hadoop, the Definitive Guide</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="ying dai" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script src='https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js'></script>
<link href='http://apps.bdimg.com/libs/highlight.js/9.1.0/styles/zenburn.min.css' rel='stylesheet'>
<script src='http://apps.bdimg.com/libs/highlight.js/9.1.0/highlight.min.js'></script>
<script>hljs.initHighlightingOnLoad();</script>
<link rel='stylesheet' href='../css/worg2.css' typbe='text/css'/>
<link rel='shortcut icon' type='image/x-icon' href='/favicon.ico'>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">

<div class='nav'>
<div class='blog' style='text-align:right'>
<a href='/index.html'> Home </a> | <a href='/contact.html'> About </a>
</div>
</div>
</div>
<div id="content">
<h1 class="title">Hadoop, the Definitive Guide</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6c19ba6">Chapter 1 初识 Hadoop</a>
<ul>
<li><a href="#org5c92b24">Hadoop 生态系统</a></li>
<li><a href="#orgc89f05b">兼容性</a></li>
</ul>
</li>
<li><a href="#orga032525">Chapter 2</a>
<ul>
<li><a href="#org5bd3715">气象数据集</a></li>
</ul>
</li>
<li><a href="#orgd53927a">Chapter 3 分布式文件系统</a>
<ul>
<li><a href="#orgbe1aa8e">HDFS 的设计</a></li>
<li><a href="#org75c0259">HDFS 的概念</a></li>
<li><a href="#org5032366">namenode 和 datanode</a></li>
<li><a href="#orga9f80c8">联邦 HDFS</a></li>
<li><a href="#orgf71b631">命令行接口</a></li>
<li><a href="#orgc41d98f">Java 接口</a></li>
<li><a href="#org80d9768">数据流</a></li>
<li><a href="#org9fa4491">一致模型</a></li>
<li><a href="#orgf4bdb78">通过 distcp 并行复制</a></li>
<li><a href="#org4286af4">Hadoop 存档 （这部分第 4 版没了）</a></li>
</ul>
</li>
<li><a href="#org926b879">Chapter 4 YARN</a>
<ul>
<li><a href="#org5c470f9">YARN 运行一个 APP</a></li>
<li><a href="#org4a2876a">YAYN scheduling</a></li>
</ul>
</li>
<li><a href="#org964a651">Chapter 5 Hadoop I/O</a></li>
<li><a href="#org0a71eae">Chapter 6 MapReduce 应用开发</a>
<ul>
<li><a href="#orgf9e2289">配置 Configuration</a></li>
</ul>
</li>
<li><a href="#orgcff64e8">MapReduce 工作机制</a></li>
<li><a href="#org87ce5b1">Chapter 12 Hive</a>
<ul>
<li><a href="#orgd3b850c">Hive shell</a></li>
<li><a href="#org8c33a61">MetaStore</a></li>
<li><a href="#org6bad97e">Hive 和 传统数据库的比较</a></li>
<li><a href="#org3bfc13a">HiveQL</a></li>
<li><a href="#org7a0039f">操作和函数</a></li>
<li><a href="#org991acad">类型转换</a></li>
<li><a href="#orgd2b07e7">表</a></li>
<li><a href="#org8d504dc">分区和桶</a></li>
<li><a href="#org572e404">存储格式</a></li>
<li><a href="#orgbda9303">导入数据</a></li>
<li><a href="#org355e1ad">查询数据</a></li>
<li><a href="#org37dc70b">调用外部脚本</a></li>
<li><a href="#orgc663148">连接</a></li>
<li><a href="#org45f708b">子查询</a></li>
<li><a href="#orgb8f41d9">视图</a></li>
<li><a href="#org2936ae9">用户定义函数</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr />
<p>
如何搭建一个 Hadoop 集群可以看这个：<a href="http://www.jianshu.com/p/c0f4d30a50b9">http://www.jianshu.com/p/c0f4d30a50b9</a>
</p>
<hr />

<div id="outline-container-org6c19ba6" class="outline-2">
<h2 id="org6c19ba6">Chapter 1 初识 Hadoop</h2>
<div class="outline-text-2" id="text-org6c19ba6">
<p>
This is a long time to read all data on a single drive—and writing is even slower. The obvious way to reduce the time is to read from multiple disks at once. Imagine if we had 100 drives, each holding one hundredth of the data. Working in parallel, we could read the data in under two minutes.
</p>

<p>
The first problem to solve is hardware failure: as soon as you
start using many pieces of hardware, the chance that one will fail
is fairly high. A common way of avoiding data loss is through
replication: redundant copies of the data are kept by the system
so that in the event of failure, there is another copy available.
</p>

<p>
总结来说，Hadoop 对数据的存储和分析提供了一种可靠的、可扩展的的平台。
</p>

<p>
为什么不能用关系型数据库来对大量硬盘上的数据进行批量分析呢？
如果数据访问模式包含大量的硬盘寻址，那么读取大量的数据集就会很慢。另一方面，如果数据库只是更新一小部分的内容，传统的 B 树更加有优势。
</p>

<p>
MapReduce 适合一次写入、多次读取的应用，关系数据库更适持续更新的数据集。
</p>


<div class="figure">
<p><img src="./imgs/20170921_120427_77207zxU.png" alt="20170921_120427_77207zxU.png" width="100%" />
</p>
</div>


<p>
MapReduce 对于非结构化数据或半结构化数据非常有效。
MapReduce 尽量在计算节点上存储数据，以实现数据的本地快速访问。数据本地化是它的核心特征。
</p>
</div>


<div id="outline-container-org5c92b24" class="outline-3">
<h3 id="org5c92b24">Hadoop 生态系统</h3>
<div class="outline-text-3" id="text-org5c92b24">
<p>
Hadoop 因 MapReduce 和 HDFS 而出名，但是它也泛指一系列相关的项目。
</p>
</div>
</div>

<div id="outline-container-orgc89f05b" class="outline-3">
<h3 id="orgc89f05b">兼容性</h3>
<div class="outline-text-3" id="text-orgc89f05b">
<p>
Hadoop 升级要考虑到兼容性：API 兼容性、数据兼容性和连接兼容性。
</p>
</div>
</div>
</div>

<div id="outline-container-orga032525" class="outline-2">
<h2 id="orga032525">Chapter 2</h2>
<div class="outline-text-2" id="text-orga032525">
</div><div id="outline-container-org5bd3715" class="outline-3">
<h3 id="org5bd3715">气象数据集</h3>
<div class="outline-text-3" id="text-org5bd3715">
<p>
这个例子中，要写一个挖掘气象数据的程序。
</p>
</div>

<ul class="org-ul"><li><a id="org9e7ab56"></a>使用 awk 处理<br /><div class="outline-text-4" id="text-org9e7ab56">
<p>
首先看下使用 awk 来处理 <a href="https://github.com/learning-from-books/hadoop-book/blob/master/ch02-mr-intro/src/main/awk/max_temperature.sh">code</a>
</p>
</div></li>

<li><a id="org900b725"></a>Hadoop 来处理<br /><div class="outline-text-4" id="text-org900b725">
<p>
map 处理输入的数据，得到 （年，温度）的键值对。
</p>

<p>
reduce 接收到 (年，[温度列表])， 然后找到最大温度。
</p>



<div class="figure">
<p><img src="./imgs/20170921_155020_77207aQn.png" alt="20170921_155020_77207aQn.png" width="100%" />
</p>
</div>


<p>
这部分的代码见 github.
</p>
</div></li>

<li><a id="org0ae82b5"></a>横向扩展<br /><div class="outline-text-4" id="text-org0ae82b5">
<p>
为了实现横向扩展，我们需要把数据存储在分布式文件系统中，一般为 HDFS。
</p>
</div></li>

<li><a id="org9f8ab1b"></a>数据流<br /><div class="outline-text-4" id="text-org9f8ab1b">
<p>
MapReduce 作业是客户端需要执行的一个工作单元：它包括输入数据、MapReduce 程序和配置信息。Hadoop 将作业分成若干个小任务来执行：map 任务和 reduce 任务。
</p>

<p>
Hadoop 将 MapReduce 的输入数据划分成等长的小数据块，称为“分片”。每个分片构建一个
map 任务。 一个合理的分片大小为 HDFS 的一个块的大小，默认 64M
</p>

<p>
Hadoop 在存储有输入数据的节点是运行 map 可以获得最佳性能。这就是所谓的“数据本地优化”。map 任务将其输出写入到本地硬盘，而非 HDFS。这是因为它的结果是中间结果，reduce 处理后才是最终结果。
所以，中间结果存到 hdfs 意义不大。
</p>

<p>
reduce 任务不具有数据本地化的优势。下面是一个 reduce 任务的 MapReduce 数据流
</p>


<div class="figure">
<p><img src="./imgs/20170921_171719_77207nat.png" alt="20170921_171719_77207nat.png" width="100%" />
</p>
</div>
</div></li>

<li><a id="orgee30f21"></a>combiner 函数<br /><div class="outline-text-4" id="text-orgee30f21">
<p>
Hadoop 允许用户针对 map 任务的输出指定一个 combiner。
combiner 的输出作为 reduce 函数的输入。combiner 属于一种优化方案。
</p>

<p>
以上面温度的例子来说明。
第一个 map：
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, 0<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 20<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 10<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>
<p>
第二个 map:
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, 25<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 15<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>

<p>
reduce 函数调用时输入:
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, <span class="org-rainbow-delimiters-depth-2">[</span>0, 20, 10, 25, 15<span class="org-rainbow-delimiters-depth-2">]</span><span class="org-rainbow-delimiters-depth-1">)</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">&#36755;&#20986;</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 25<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>

<p>
但是我们如果用了 combiner 以后，我们就可以提前找到每个 map 任务的输出结果的最高气温。现在 reduce 的输入变成了。
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, <span class="org-rainbow-delimiters-depth-2">[</span>20, 25<span class="org-rainbow-delimiters-depth-2">]</span><span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>
</div></li>

<li><a id="org310a31a"></a>Hadoop Streaming<br /><div class="outline-text-4" id="text-org310a31a">
<p>
它提供了 MapReduce 的 API，我们可以用其他的语言来写 map 和 reduce。
</p>
</div></li></ul>
</div>
</div>

<div id="outline-container-orgd53927a" class="outline-2">
<h2 id="orgd53927a">Chapter 3 分布式文件系统</h2>
<div class="outline-text-2" id="text-orgd53927a">
<p>
当数据集的大小超过一台独立的物理计算机的存储能力时，有必要对它分区并存储到若干的计算机上，也就需要一个分布式文件系统。
</p>

<p>
这部分主要介绍 HDFS 文件系统。
</p>
</div>


<div id="outline-container-orgbe1aa8e" class="outline-3">
<h3 id="orgbe1aa8e">HDFS 的设计</h3>
<div class="outline-text-3" id="text-orgbe1aa8e">
<p>
HDFS 流式数据访问模式来存储超大文件。
</p>
<ul class="org-ul">
<li>超大文件</li>
<li><p>
流式数据访问。
</p>

<p>
一次写入，多次读取时最高效的访问模式。
</p></li>
<li><p>
商用硬件
</p>

<p>
Hadoop 并不需要运行在昂贵且可靠的硬件上。
</p></li>
<li><p>
低延迟的数据访问
</p>

<p>
要求低时间的应用不适合 HDFS, 它是为高吞吐量应用优化的。
</p></li>
<li><p>
大量的小文件
</p>

<p>
namenode 将文件系统的元数据存储在内存中，因此文件系统的文件总数受限于 namenode 的内存容量。
每个文件、目录和数据块的存储信息约占 150B。
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org75c0259" class="outline-3">
<h3 id="org75c0259">HDFS 的概念</h3>
<div class="outline-text-3" id="text-org75c0259">
<p>
HDFS 的块要比磁盘默认的块大很多，如果一个小于块大小的的文件不会占据整个的块空间。
</p>

<p>
为什么 HDFS 块这么大？原因是为了最小化寻址开销。例如，如果寻址时间为 10ms，而传输速率为 100MB/s，为了使寻址时间仅占用 1%，我们要将块的大小设置为大约 100MB。
</p>

<p>
块非常适合数据备份进而提供数据容错能力和提高可用性。将每个块复制到少数几个独立的机器上，可以确保块、磁盘或者机器发生故障后数据的恢复。
</p>

<p>
HDFS 中的 fsck 指令可以显示块信息：
</p>
<div class="container">
<pre><code class="shell">hadoop fsck / -files -blocks
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org5032366" class="outline-3">
<h3 id="org5032366">namenode 和 datanode</h3>
<div class="outline-text-3" id="text-org5032366">
<p>
HDFS 有两类节点以管理者-工作者模式运行，一个是 namenode，一个 datanode.
namenode 维护文件系统树以及所有的文件和目录。他们通过：命名空间镜像文件和编辑日志文件来永久的保存在本地的本地磁盘上。
</p>

<p>
客户端(client) 代表用户通过与 namenode 和 datanode 交互来访问整个文件系统。
datanode 是系统的工作节点，它根据需要存储并检索数据块，并定期向 namenode 发送他们所存储的块的列表。
如果 namenode 机器毁掉，我们的文件将会丢失，因为我们无法重建我们的数据。Hadoop 提供了两种方式：
</p>
<ul class="org-ul">
<li>备份元数据持久状态的文件</li>
<li>运行一个辅助的 namenode, 但它不能被用作 namenode。</li>
</ul>
</div>
</div>
<div id="outline-container-orga9f80c8" class="outline-3">
<h3 id="orga9f80c8">联邦 HDFS</h3>
<div class="outline-text-3" id="text-orga9f80c8">
<p>
在联邦环境下，有多个的 namenode，它们管理不同的命名空间，它们之间互相不影响。
</p>
</div>
</div>
<div id="outline-container-orgf71b631" class="outline-3">
<h3 id="orgf71b631">命令行接口</h3>
<div class="outline-text-3" id="text-orgf71b631">
<p>
在配置中，我们需要设置 fs.default.name，例如 hdfs://localhost/, 用于设置 Hadoop 的默认文件系统。
第二个属性，dfs.replication, 设置为 1，这样它就不会将文件系统块复制 3 份了。
</p>

<p>
文件系统的基本操作
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/quangle.txt
</code></pre>
</div>

<p>
我们其实可以简化这个操作
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt /user/quangle.txt
</code></pre>
</div>

<p>
也可以使用相对路径, 复制到 HOMΕ 中
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt
</code></pre>
</div>

<p>
还有
</p>
<div class="container">
<pre><code class="shell">hadoop fs -mkdir ...
hadoop fs -copyToLocal ...
</code></pre>
</div>
</div>
</div>
<div id="outline-container-orgc41d98f" class="outline-3">
<h3 id="orgc41d98f">Java 接口</h3>
<div class="outline-text-3" id="text-orgc41d98f">
<p>
从 Hadoop URL 读取数据
</p>
<div class="container">
<pre><code class="java"><span class="org-type">InputStream</span> <span class="org-variable-name">in</span> = <span class="org-constant">null</span>;
<span class="org-keyword">try</span> <span class="org-rainbow-delimiters-depth-1">{</span>
    in = <span class="org-keyword">new</span> <span class="org-type">URL</span><span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"hdfs://host/path"</span><span class="org-rainbow-delimiters-depth-2">)</span>.openStream<span class="org-rainbow-delimiters-depth-2">()</span>;
    <span class="org-comment-delimiter">// </span><span class="org-comment">process in</span>
<span class="org-rainbow-delimiters-depth-1">}</span> <span class="org-keyword">finally</span> <span class="org-rainbow-delimiters-depth-1">{</span>
    IOUtils.closeStream<span class="org-rainbow-delimiters-depth-2">(</span>in<span class="org-rainbow-delimiters-depth-2">)</span>;
<span class="org-rainbow-delimiters-depth-1">}</span>
</code></pre>
</div>

<p>
通过 URLStreamHandler 实例以标准输出方式显示 Hadoop 文件
</p>

<div class="container">
<pre><code class="java"><span class="org-keyword">public</span> <span class="org-keyword">class</span> <span class="org-type">URLCat</span><span class="org-rainbow-delimiters-depth-1">{</span>
    <span class="org-keyword">static</span> <span class="org-rainbow-delimiters-depth-2">{</span>
        URL.setURLStreamHandlerFactory<span class="org-rainbow-delimiters-depth-3">(</span><span class="org-keyword">new</span> <span class="org-type">FsUrlStreamHandlerFactory</span><span class="org-rainbow-delimiters-depth-4">()</span><span class="org-rainbow-delimiters-depth-3">)</span>;
    <span class="org-rainbow-delimiters-depth-2">}</span>
    <span class="org-keyword">public</span> <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-function-name">main</span><span class="org-rainbow-delimiters-depth-2">(</span><span class="org-type">String</span><span class="org-rainbow-delimiters-depth-3">[]</span> <span class="org-variable-name">args</span><span class="org-rainbow-delimiters-depth-2">)</span> <span class="org-keyword">throws</span> <span class="org-type">Exception</span> <span class="org-rainbow-delimiters-depth-2">{</span>
        <span class="org-type">InputStream</span> <span class="org-variable-name">in</span> = <span class="org-constant">null</span>;
        <span class="org-keyword">try</span> <span class="org-rainbow-delimiters-depth-3">{</span>
            in = <span class="org-keyword">new</span> <span class="org-type">URL</span><span class="org-rainbow-delimiters-depth-4">(</span>args<span class="org-rainbow-delimiters-depth-5">[</span>0<span class="org-rainbow-delimiters-depth-5">]</span>.openStream<span class="org-rainbow-delimiters-depth-5">()</span><span class="org-rainbow-delimiters-depth-4">)</span>;
            IOUtils.copyBytes<span class="org-rainbow-delimiters-depth-4">(</span>in, System.out, 4096, <span class="org-constant">false</span><span class="org-rainbow-delimiters-depth-4">)</span>;
        <span class="org-rainbow-delimiters-depth-3">}</span> <span class="org-keyword">finally</span> <span class="org-rainbow-delimiters-depth-3">{</span>
            IOUtils.closeStream<span class="org-rainbow-delimiters-depth-4">(</span>in<span class="org-rainbow-delimiters-depth-4">)</span>;
        <span class="org-rainbow-delimiters-depth-3">}</span>
    <span class="org-rainbow-delimiters-depth-2">}</span>
<span class="org-rainbow-delimiters-depth-1">}</span>
</code></pre>
</div>

<p>
我们可这样运行
</p>
<div class="container">
<pre><code class="shell">hadoop URLCat hdfs://localhost/user/tom/quangle.txt
</code></pre>
</div>
</div>
</div>
<div id="outline-container-org80d9768" class="outline-3">
<h3 id="org80d9768">数据流</h3>
<div class="outline-text-3" id="text-org80d9768">
<p>
下面先看文件的读取。
为了了解客户端以及与之交互的 HDFS、namenode 和 datanode
之间的数据流是什么样的，我们看下图。
</p>

<div class="figure">
<p><img src="./imgs/20170922_210837_77207z4I.png" alt="20170922_210837_77207z4I.png" width="100%" />
</p>
</div>

<p>
客户端通过调用 FileSyste 对象的 open() 方法来打开要读取的文件，
对于 HDFS 来说，这个对象是分布式文件系统的一个实例。分布式文件系统通过
RPC 来调用 namenode，以确定文件块的起始位置。对于每一个块，namenode 返回存有该块副本的 datanode 地址。客户端读取数据，块是按照打开 DFSInputStream 与 datanode
新建连接的顺序读取的。
</p>

<p>
如果 DFSInputStream 在于 datanode 通信时遇到错误，它会尝试从这个快的另外一个最邻近的 datanode 读取数据。
它也记住那个故障 datanode，以保证以后不会反复读取该节点上后续的块。这个设计的一个重点是，namenode 告知客户端每个块的最佳 datanode，并让客户端直接连接到 datanode 检索数据。这里有个节点“彼此近邻”的问题？如何定义它？
一个简单的方式：把网络看做一个树，两个节点间的距离是它们到最近共同祖先的距离和。
</p>


<p>
下面看下文件的写入。
</p>

<p>
我们要考虑的是如何新建一个文件，把数据写入到文件，最后关闭该文件。
</p>

<div class="figure">
<p><img src="./imgs/20170922_214041_77207ADP.png" alt="20170922_214041_77207ADP.png" width="100%" />
</p>
</div>

<p>
首先 namenode 会执行各种的检查来确保这个文件不存在以及客户端有新建文件的权限。
DitributedFileSytem 向客户端返回一个 FSDataOutputStream 对象，客户端开始写入数据。
注意其中的 4、5 的过程，其中三个 datanode 是节点的副本，默认为 3 个。
</p>


<div class="figure">
<p><img src="./imgs/20170923_133041_77207nhh.png" alt="20170923_133041_77207nhh.png" width="40%" />
</p>
</div>

<p>
上图解释了，我们应该如何去放复本。默认策略时候运行客户端的节点上放第一个复本。（如果客户端运行在集群外，就随机选择一个节点，但是要避免太满或太忙的节点）。第二个选在与第一个不同且随机另外选择的机架。
</p>
</div>
</div>
<div id="outline-container-org9fa4491" class="outline-3">
<h3 id="org9fa4491">一致模型</h3>
<div class="outline-text-3" id="text-org9fa4491">
<p>
新建文件以后能够立即可见，但是写入文件内容不能保证立即可见。只有在写入超过一个块对新的 reader 才可见。
但是我们可以通过使用 sync() 来使得所有缓存与数据节点强行同步。
</p>

<p>
在 HDFS 中关闭文件其实还隐含执行 sync() 方法。
</p>
<div class="container">
<pre><code class="java"><span class="org-type">Path</span> <span class="org-variable-name">p</span> = <span class="org-keyword">new</span> <span class="org-type">Path</span><span class="org-rainbow-delimiters-depth-1">(</span><span class="org-string">"p"</span><span class="org-rainbow-delimiters-depth-1">)</span>
OutputStream out = fs.create<span class="org-rainbow-delimiters-depth-1">(</span>p<span class="org-rainbow-delimiters-depth-1">)</span>;
out.write<span class="org-rainbow-delimiters-depth-1">(</span><span class="org-string">"content"</span>.getBytes<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"UTF-8"</span><span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>;
out.close<span class="org-rainbow-delimiters-depth-1">()</span>;
assertThat<span class="org-rainbow-delimiters-depth-1">(</span>fs.getFileStatus<span class="org-rainbow-delimiters-depth-2">(</span>p<span class="org-rainbow-delimiters-depth-2">)</span>.getLen<span class="org-rainbow-delimiters-depth-2">()</span>, is<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-rainbow-delimiters-depth-3">(</span><span class="org-rainbow-delimiters-depth-4">(</span><span class="org-type">long</span><span class="org-rainbow-delimiters-depth-4">)</span> <span class="org-string">"content"</span>.length<span class="org-rainbow-delimiters-depth-4">()</span><span class="org-rainbow-delimiters-depth-3">)</span><span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>
</div>
</div>
<div id="outline-container-orgf4bdb78" class="outline-3">
<h3 id="orgf4bdb78">通过 distcp 并行复制</h3>
<div class="outline-text-3" id="text-orgf4bdb78">
<p>
distcp 是一个分布式复制程序，它可以从 Hadoop 中复制大量的数据，
也可以将大量的数据复制到 Hadoop 中。它的一个典型的应用是在两个 Hadoop 集群间复制数据。
</p>

<p>
例如
</p>
<div class="container">
<pre><code class="shell">hadoop distcp hdfs://namenode1/foo hdfs://namenode2/bar
</code></pre>
</div>

<p>
这个指令把第一个集群的 /foo 目录复制到第二个集群的 /bar 目录下，所以第二个集群的目录变为了
/bar/foo。
</p>

<p>
distcp 是作为一个 MapReduce 作业来实现的，这里没有 reducer, 只有 map。
</p>

<p>
一个有效的替换是对
</p>
<div class="container">
<pre><code class="shell">hadoop distcp file1 file2
<span class="org-comment-delimiter"># </span><span class="org-comment">&#26367;&#25442;</span>
hadoop fs -cp
</code></pre>
</div>

<p>
也可以对文件夹操作
</p>
<div class="container">
<pre><code class="shell">hadoop distcp dir1 dir2
<span class="org-comment-delimiter"># </span><span class="org-comment">if dir2 does not exist, it will be created.</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">else it will be dir2/dir2</span>
<span class="org-comment-delimiter"># </span><span class="org-comment">if you dont want it, you can use -overwrite option</span>
</code></pre>
</div>

<p>
我们可以使用 update 来更新那些改变的文件
</p>
<div class="container">
<pre><code class="shell">hadoop distcp -update dir1 dir2
</code></pre>
</div>

<p>
如果两个集群的 HDFS 版本不同，我们可以使用 webhdfs 来代替前面的。
</p>
</div>
</div>
<div id="outline-container-org4286af4" class="outline-3">
<h3 id="org4286af4">Hadoop 存档 （这部分第 4 版没了）</h3>
<div class="outline-text-3" id="text-org4286af4">
<p>
每个块存到 namenode 的内存中，因此如果 Hadoop 来存小文件是非常不适合的。
</p>

<div class="container">
<pre><code class="shell">hadoop archive -archiveName file.har /my/files /my
</code></pre>
</div>

<p>
删除一个 har 文件
</p>
<div class="container">
<pre><code class="shell">hadoop fs -rmr /my/files.har
</code></pre>
</div>
</div>
</div>
</div>
<div id="outline-container-org926b879" class="outline-2">
<h2 id="org926b879">Chapter 4 YARN</h2>
<div class="outline-text-2" id="text-org926b879">
<p>
YARN 是 Hadoop 集群的资源管理系统。YARN 提供了一套 API，但是用户一般不会直接调用。用户一般会调用基于 YARN 的上层的服务的 API，资源管理的细节对用户都是透明的。
</p>


<div class="figure">
<p><img src="./imgs/20170923_143710_772070rn.png" alt="20170923_143710_772070rn.png" width="60%" />
</p>
</div>

<p>
看上图，我们应该很清楚这种架构设计。
这一章，我们会对 YARN 有一个大体的了解，对于后面学习 Hadoop 分布式处理框架很有帮助。
</p>
</div>
<div id="outline-container-org5c470f9" class="outline-3">
<h3 id="org5c470f9">YARN 运行一个 APP</h3>
<div class="outline-text-3" id="text-org5c470f9">
<p>
YARN 主要通过两个长期运行的服务：一个资源管理器（每个集群一个）和 node 管理器（每个节点上都会有），它用来监控和启动容器（指的是 Unix process 或一个 Linux cgroup）。
</p>


<div class="figure">
<p><img src="./imgs/20170923_144450_77207AKD.png" alt="20170923_144450_77207AKD.png" width="60%" />
</p>
</div>
</div>
</div>
<div id="outline-container-org4a2876a" class="outline-3">
<h3 id="org4a2876a">YAYN scheduling</h3>
<div class="outline-text-3" id="text-org4a2876a">
<ul class="org-ul">
<li>FIFO</li>
<li>Capacity Scheduler</li>
<li>Fair Scheduler</li>
</ul>


<div class="figure">
<p><img src="./imgs/20170924_000156_77207aeP.png" alt="20170924_000156_77207aeP.png" width="50%" />
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org964a651" class="outline-2">
<h2 id="org964a651">Chapter 5 Hadoop I/O</h2>
</div>
<div id="outline-container-org0a71eae" class="outline-2">
<h2 id="org0a71eae">Chapter 6 MapReduce 应用开发</h2>
<div class="outline-text-2" id="text-org0a71eae">
<p>
它的编写流程如下：
</p>
<ul class="org-ul">
<li>map 函数</li>
<li>reduce 函数</li>
<li>单元测试</li>
<li>驱动程序测试</li>
<li>集群调试</li>
<li>优化</li>
</ul>
</div>

<div id="outline-container-orgf9e2289" class="outline-3">
<h3 id="orgf9e2289">配置 Configuration</h3>
<div class="outline-text-3" id="text-orgf9e2289">
<p>
我们可以先定义配置文件
</p>

<div class="container">
<pre><code class="java"><span class="org-type">Configuration</span> <span class="org-variable-name">conf</span> = <span class="org-keyword">new</span> <span class="org-type">Configuration</span><span class="org-rainbow-delimiters-depth-1">()</span>;
conf.addResource<span class="org-rainbow-delimiters-depth-1">(</span><span class="org-string">"configuration-1.xml"</span><span class="org-rainbow-delimiters-depth-1">)</span>;
assertThat<span class="org-rainbow-delimiters-depth-1">(</span>conf.get<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"color"</span><span class="org-rainbow-delimiters-depth-2">)</span>, is<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"yellow"</span><span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>;
assertThat<span class="org-rainbow-delimiters-depth-1">(</span>conf.getInt<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"size"</span>, 0<span class="org-rainbow-delimiters-depth-2">)</span>, is<span class="org-rainbow-delimiters-depth-2">(</span>10<span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>;
assertThat<span class="org-rainbow-delimiters-depth-1">(</span>conf.get<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"breadth"</span>, <span class="org-string">"wide"</span><span class="org-rainbow-delimiters-depth-2">)</span>, is<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"wide"</span><span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
我们可以使用多个配置文件，后面的会覆盖前面的，如果被标记了 final
</p>
</div>
</div>
</div>

<div id="outline-container-orgcff64e8" class="outline-2">
<h2 id="orgcff64e8">MapReduce 工作机制</h2>
</div>

<div id="outline-container-org87ce5b1" class="outline-2">
<h2 id="org87ce5b1">Chapter 12 Hive</h2>
<div class="outline-text-2" id="text-org87ce5b1">
<p>
Hive 安装没搞定，用的 Docker 来搞的。。。
</p>
</div>

<div id="outline-container-orgd3b850c" class="outline-3">
<h3 id="orgd3b850c">Hive shell</h3>
<div class="outline-text-3" id="text-orgd3b850c">
<div class="container">
<pre><code class="shell">show tables;
</code></pre>
</div>

<p>
首先 Hive 大小写不敏感，同时我们可以使用 Tab 来补全关键字和函数。
</p>

<p>
我们也可以指定文件
</p>
<div class="container">
<pre><code class="sql">hive -f script.q
</code></pre>
</div>

<p>
我们也可以这样, 不用末尾分号
</p>
<div class="container">
<pre><code class="sql">hive -e "<span class="org-keyword">select</span> * <span class="org-keyword">from</span> store_sales"

<span class="org-comment">-- &#19979;&#38754;&#20250;&#25490;&#38500;&#26597;&#35810;&#32467;&#26524;&#20043;&#22806;&#30340;&#20449;&#24687;</span>
hive -S -e ""
</code></pre>
</div>

<p>
下面，我们举一个具体的例子，就是前面我们的天气的数据。
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">create</span> <span class="org-keyword">table</span> <span class="org-function-name">records</span> <span class="org-rainbow-delimiters-depth-1">(</span><span class="org-keyword">year</span> string, temperature <span class="org-type">int</span>, quality <span class="org-type">int</span><span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-type">row</span> format delimited fields terminated <span class="org-keyword">by</span> <span class="org-string">'\t'</span>;
</code></pre>
</div>
<p>
这样我们就创建了一个 records 表。
</p>

<p>
然后我们将 sample 数据导入到 records 表中。数据我们可以在<a href="https://github.com/tomwhite/hadoop-book/blob/master/input/ncdc/micro-tab/sample.txt">这里</a>找到。
</p>

<div class="container">
<pre><code class="shell">load data local inpath <span class="org-string">'sample.txt'</span> overwrite into table records;
</code></pre>
</div>

<p>
我们查看一下 records 表的内容就可以看到数据了。
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">select</span> * <span class="org-keyword">from</span> records;
<span class="org-comment">-- &#25214;&#21040;&#27599;&#24180;&#30340;&#26368;&#39640;&#28201;&#24230;</span>
<span class="org-keyword">select</span> <span class="org-keyword">year</span>, <span class="org-builtin">max</span><span class="org-rainbow-delimiters-depth-1">(</span>temperature<span class="org-rainbow-delimiters-depth-1">)</span> <span class="org-keyword">from</span> records <span class="org-keyword">group</span> <span class="org-keyword">by</span> <span class="org-keyword">year</span>;
</code></pre>
</div>

<p>
下面看下 Hive 的体系结构
</p>

<div class="figure">
<p><img src="./imgs/20171013_002739_2696RBc.png" alt="20171013_002739_2696RBc.png" width="100%" />
</p>
</div>
</div>
</div>


<div id="outline-container-org8c33a61" class="outline-3">
<h3 id="org8c33a61">MetaStore</h3>
<div class="outline-text-3" id="text-org8c33a61">

<div class="figure">
<p><img src="./imgs/20171013_003452_2696eLi.png" alt="20171013_003452_2696eLi.png" width="100%" />
</p>
</div>

<p>
MetaStore 是 Hive 元数据的存储位置。MetaStore 分为两个部分：服务和后台数据的存储。
默认情况，也就是内嵌模式，metastore 服务和 hive 服务运行在同一个 JVM 中。后面的分别是本地 metastore
和远程 metastore。只有远程 metastore 中的 metastore 服务和 hive 服务不是运行在同一个 JVM 中。
</p>
</div>
</div>


<div id="outline-container-org6bad97e" class="outline-3">
<h3 id="org6bad97e">Hive 和 传统数据库的比较</h3>
<div class="outline-text-3" id="text-org6bad97e">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">读时模式</th>
<th scope="col" class="org-left">写时模式</th>
<th scope="col" class="org-left">更新</th>
<th scope="col" class="org-left">事务</th>
<th scope="col" class="org-left">索引</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">Hive</td>
<td class="org-left">支持</td>
<td class="org-left">不支持</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">传统</td>
<td class="org-left">不支持</td>
<td class="org-left">支持</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>


<div id="outline-container-org3bfc13a" class="outline-3">
<h3 id="org3bfc13a">HiveQL</h3>
<div class="outline-text-3" id="text-org3bfc13a">
<p>
下面是它们的一些区别：
</p>

<div class="figure">
<p><img src="./imgs/20171013_101848_2696rVo.png" alt="20171013_101848_2696rVo.png" width="100%" />
</p>
</div>

<p>
Hive 同时支持很多的数据类型，比较特殊的是它还支持数组类型：
</p>

<div class="figure">
<p><img src="./imgs/20171013_102231_26964fu.png" alt="20171013_102231_26964fu.png" width="100%" />
</p>
</div>

<p>
上面是一些基本类型，下面是一些比较复杂的类型
</p>

<div class="figure">
<p><img src="./imgs/20171013_102332_2696Fq0.png" alt="20171013_102332_2696Fq0.png" width="100%" />
</p>
</div>


<p>
对于复杂类型，我们举个例子就很好理解了：
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">create</span> <span class="org-keyword">table</span> <span class="org-function-name">complex</span> <span class="org-rainbow-delimiters-depth-1">(</span>
    c1 <span class="org-type">array</span>&lt;<span class="org-type">int</span>&gt;,
    c2 <span class="org-keyword">map</span>&lt;string, <span class="org-type">int</span>&gt;,
    c3 struct&lt;a:string, b:<span class="org-type">int</span>, c:<span class="org-type">double</span>&gt;,
    c4 uniontype&lt;string, <span class="org-type">int</span>&gt;
<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
然后后我们可以
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">select</span> c1<span class="org-rainbow-delimiters-depth-1">[</span>0<span class="org-rainbow-delimiters-depth-1">]</span>, c2<span class="org-rainbow-delimiters-depth-1">[</span><span class="org-string">'b'</span><span class="org-rainbow-delimiters-depth-1">]</span>, c3.c, c4 <span class="org-keyword">from</span> complex;
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org7a0039f" class="outline-3">
<h3 id="org7a0039f">操作和函数</h3>
<div class="outline-text-3" id="text-org7a0039f">
<p>
我们在 Hive 中通过命令
</p>
<div class="container">
<pre><code class="sql">show functions;
<span class="org-keyword">describe</span> <span class="org-keyword">function</span> &lt;func-<span class="org-keyword">name</span>&gt;;
</code></pre>
</div>
<p>
可以查看支持的命令，一共有 216 个。
</p>
</div>
</div>

<div id="outline-container-org991acad" class="outline-3">
<h3 id="org991acad">类型转换</h3>
<div class="outline-text-3" id="text-org991acad">
<p>
Hive 会做一个隐式类型转换，它的原则是将小的范围向更大范围进行转换。
</p>
</div>
</div>

<div id="outline-container-orgd2b07e7" class="outline-3">
<h3 id="orgd2b07e7">表</h3>
<div class="outline-text-3" id="text-orgd2b07e7">
<p>
Hive 表中内容一般存在 HDFS 中，或者本地文件系统或 s3 这样的存储中。
在 Hive 中创建一个表，默认是 Hive 管理表中的内容，也叫 <b>托管表</b> 。
在加载数据的时候它会把数据移动到它的仓库目录中。
</p>

<p>
外部表的数据有自己来控制。
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">CREATE</span> <span class="org-keyword">EXTERNAL</span> <span class="org-keyword">TABLE</span> <span class="org-function-name">external_table</span> <span class="org-rainbow-delimiters-depth-1">(</span>dummy STRING<span class="org-rainbow-delimiters-depth-1">)</span> LOCATION <span class="org-string">'/user/tom/external_table'</span>;
LOAD <span class="org-keyword">DATA</span> INPATH <span class="org-string">'/user/tom/data.txt'</span> <span class="org-keyword">INTO</span> <span class="org-keyword">TABLE</span> external_table;
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org8d504dc" class="outline-3">
<h3 id="org8d504dc">分区和桶</h3>
<div class="outline-text-3" id="text-org8d504dc">
<p>
Hive 把表组织成“分区”。这是一种根据分区列（partition column）来对表进行粗略划分的机制，可以加快查询的速度。
表或者分区可以进一步划分为“桶”。
</p>

<ol class="org-ol">
<li><p>
分区
</p>

<p>
对分区使用的一个好的例子是日志。我们可以对日志按照日期进行分区，比如每天的日志在一个分区中，这样查询就会非常的高效。
对表分区不限于一个维度，我们可以有多个，例如国家。
在查询的时候分区，不会影响查询的速度。
</p>

<p>
分区是通过 PARTITIONED BY 来定义的，下面是假想的日志记录
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">CREATE</span> <span class="org-keyword">TABLE</span> <span class="org-function-name">logs</span> <span class="org-rainbow-delimiters-depth-1">(</span>ts BIGINT, line STRING<span class="org-rainbow-delimiters-depth-1">)</span>
PARTITIONED <span class="org-keyword">BY</span> <span class="org-rainbow-delimiters-depth-1">(</span>dt STRING, country STRING<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
我们把数据加载到数据表中，要显式指定加载的分区值:
</p>
<div class="container">
<pre><code class="sql">LOAD <span class="org-keyword">DATA</span> <span class="org-keyword">LOCAL</span> INPATH <span class="org-string">'input/hive/partitions/file1'</span>
<span class="org-keyword">INTO</span> <span class="org-keyword">TABLE</span> logs
PARTITION <span class="org-rainbow-delimiters-depth-1">(</span>dt=<span class="org-string">'2001-01-01'</span>, country=<span class="org-string">'GB'</span><span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
文件加载到 logs 表后，结构可能如下：
</p>

<div class="figure">
<p><img src="./imgs/20171013_111258_26963zD.png" alt="20171013_111258_26963zD.png" width="50%" />
</p>
</div>

<p>
我们可以通过命令查看分区
</p>
<div class="container">
<pre><code class="sql">SHOW PARTITIONS logs;
<span class="org-comment">-- &#32467;&#26524;</span>
dt=2001-01-01/country=GB
dt=2001-01-01/country=US
dt=2001-01-02/country=GB
dt=2001-01-02/country=US
</code></pre>
</div>

<p>
在 select 中我们可以直接使用
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> ts, dt, line
<span class="org-keyword">FROM</span> logs
<span class="org-keyword">WHERE</span> country=<span class="org-string">'GB'</span>;
</code></pre>
</div></li>

<li><p>
桶
</p>

<p>
使用桶的理由有两个：一个是高效；另一个是方便“取样”。取样可以方便我们在更小的数据集上运行测试。
</p>

<p>
我们使用 CLUSTERED BY 来划分桶使用的列和要划分的桶的个数。
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">CREATE</span> <span class="org-keyword">TABLE</span> <span class="org-function-name">bucketed_users</span> <span class="org-rainbow-delimiters-depth-1">(</span>id <span class="org-type">INT</span>, <span class="org-keyword">name</span> STRING<span class="org-rainbow-delimiters-depth-1">)</span>
CLUSTERED <span class="org-keyword">BY</span> <span class="org-rainbow-delimiters-depth-1">(</span>id<span class="org-rainbow-delimiters-depth-1">)</span> <span class="org-keyword">INTO</span> 4 BUCKETS;
</code></pre>
</div>

<p>
Hive 会根据 Id 来 hash，然后根据桶的个数取余。
我们还可以对桶根据某个列排序
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">CREATE</span> <span class="org-keyword">TABLE</span> <span class="org-function-name">bucketed_users</span> <span class="org-rainbow-delimiters-depth-1">(</span>id <span class="org-type">INT</span>, <span class="org-keyword">name</span> STRING<span class="org-rainbow-delimiters-depth-1">)</span>
CLUSTERED <span class="org-keyword">BY</span> <span class="org-rainbow-delimiters-depth-1">(</span>id<span class="org-rainbow-delimiters-depth-1">)</span> SORTED <span class="org-keyword">BY</span> <span class="org-rainbow-delimiters-depth-1">(</span>id <span class="org-keyword">ASC</span><span class="org-rainbow-delimiters-depth-1">)</span> <span class="org-keyword">INTO</span> 4 BUCKETS;
</code></pre>
</div>

<p>
这样对每个桶的连接可以变成了归并排序。
</p>

<p>
将没有分桶的数据插入到分桶的表中：
</p>
<div class="container">
<pre><code class="sql"><span class="org-comment">-- need set hive.enforce.bucketing=true</span>
<span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> bucketed_users
<span class="org-keyword">SELECT</span> * <span class="org-keyword">FROM</span> users;
</code></pre>
</div>

<p>
这里有一个用法
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> * <span class="org-keyword">FROM</span> bucketed_users
TABLESAMPLE<span class="org-rainbow-delimiters-depth-1">(</span>BUCKET 1 <span class="org-keyword">OUT</span> <span class="org-keyword">OF</span> 2 <span class="org-keyword">ON</span> id<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
我们可能不明白 out of 是干啥的？<a href="https://stackoverflow.com/questions/18781869/hive-buckets-understanding-tablesamplebucket-x-out-of-y">这里</a>有清楚的解释。
</p>
<blockquote>
<p>
When you create the table and bucket it using the clustered by
clause into 32 buckets (as an example), hive buckets your data
into 32 buckets using deterministic hash functions. Then when
you use TABLESAMPLE(BUCKET x OUT OF y), hive divides your
buckets into groups of y buckets and then picks the x'th
bucket of each group.
</p>
</blockquote></li>
</ol>
</div>
</div>

<div id="outline-container-org572e404" class="outline-3">
<h3 id="org572e404">存储格式</h3>
<div class="outline-text-3" id="text-org572e404">
<p>
存储格式分为“行格式”和“文件格式”。
</p>

<p>
行格式默认使用的是 Control-A。
</p>
</div>
</div>

<div id="outline-container-orgbda9303" class="outline-3">
<h3 id="orgbda9303">导入数据</h3>
<div class="outline-text-3" id="text-orgbda9303">
<ol class="org-ol">
<li><p>
insert
</p>

<div class="container">
<pre><code class="sql"><span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> target
<span class="org-keyword">SELECT</span> col1, col2
<span class="org-keyword">FROM</span> <span class="org-keyword">source</span>;
</code></pre>
</div>

<p>
或者
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> target
PARTITION <span class="org-rainbow-delimiters-depth-1">(</span>dt=<span class="org-string">'2001-01-01'</span><span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-keyword">SELECT</span> col1, col2
<span class="org-keyword">FROM</span> <span class="org-keyword">source</span>;
</code></pre>
</div>

<p>
上面使用了 overwrite 这样会吧 target 表覆盖掉。如果不想这样可以使用
insert into table。
</p>

<p>
我们还可以使用动态分区来插入表：
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> target
PARTITION <span class="org-rainbow-delimiters-depth-1">(</span>dt<span class="org-rainbow-delimiters-depth-1">)</span> <span class="org-keyword">SELECT</span> col1, col2, dt
<span class="org-keyword">FROM</span> <span class="org-keyword">source</span>;
</code></pre>
</div></li>

<li><p>
多表插入我们可以把 FROΜ 放到前面
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">FROM</span> <span class="org-keyword">source</span>
<span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> target
<span class="org-keyword">SELECT</span> col1, col2;
</code></pre>
</div>

<div class="container">
<pre><code class="sql"><span class="org-keyword">FROM</span> records2
<span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> stations_by_year
  <span class="org-keyword">SELECT</span> <span class="org-keyword">year</span>, <span class="org-builtin">COUNT</span><span class="org-rainbow-delimiters-depth-1">(</span><span class="org-keyword">DISTINCT</span> station<span class="org-rainbow-delimiters-depth-1">)</span>
  <span class="org-keyword">GROUP</span> <span class="org-keyword">BY</span> <span class="org-keyword">year</span>
<span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> records_by_year
  <span class="org-keyword">SELECT</span> <span class="org-keyword">year</span>, <span class="org-builtin">COUNT</span><span class="org-rainbow-delimiters-depth-1">(</span>1<span class="org-rainbow-delimiters-depth-1">)</span>
  <span class="org-keyword">GROUP</span> <span class="org-keyword">BY</span> <span class="org-keyword">year</span>
<span class="org-keyword">INSERT</span> OVERWRITE <span class="org-keyword">TABLE</span> good_records_by_year
  <span class="org-keyword">SELECT</span> <span class="org-keyword">year</span>, <span class="org-builtin">COUNT</span><span class="org-rainbow-delimiters-depth-1">(</span>1<span class="org-rainbow-delimiters-depth-1">)</span>
  <span class="org-keyword">WHERE</span> temperature != 9999 <span class="org-keyword">AND</span> quality <span class="org-keyword">IN</span> <span class="org-rainbow-delimiters-depth-1">(</span>0, 1, 4, 5, 9<span class="org-rainbow-delimiters-depth-1">)</span>
  <span class="org-keyword">GROUP</span> <span class="org-keyword">BY</span> <span class="org-keyword">year</span>;
</code></pre>
</div></li>

<li><p>
ℂΤΑ𝕊 操作
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">create</span> <span class="org-keyword">table</span> <span class="org-function-name">target</span>
<span class="org-keyword">as</span>
<span class="org-keyword">select</span> col1, col2
<span class="org-keyword">from</span> <span class="org-keyword">source</span>;
</code></pre>
</div>

<p>
这个操作是原子操作，如果 select 失败，这表不会创建。
</p></li>

<li><p>
表的修改
</p>
<div class="container">
<pre><code class="sql"><span class="org-comment">-- alter table name</span>
<span class="org-keyword">alter</span> <span class="org-keyword">table</span> <span class="org-keyword">source</span> rename <span class="org-keyword">to</span> target;
<span class="org-comment">-- add new column</span>
<span class="org-keyword">alter</span> <span class="org-keyword">table</span> <span class="org-function-name">target</span> <span class="org-keyword">add</span> columns <span class="org-rainbow-delimiters-depth-1">(</span>col3 string<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div></li>

<li>表的删除
drop table
如果是内部表，会删除数据和元数据。如果是外部表会删除元数据。</li>
</ol>
</div>
</div>

<div id="outline-container-org355e1ad" class="outline-3">
<h3 id="org355e1ad">查询数据</h3>
<div class="outline-text-3" id="text-org355e1ad">
<p>
我们可以使用 order by 来全局排序，但是这种排序只用一个 reducer 来做，效率非常的低。
我们其实大部分是不需要全局排序的，这时可以使用 sort by。
</p>

<div class="container">
<pre><code class="sql"><span class="org-keyword">from</span> records2
<span class="org-keyword">select</span> <span class="org-keyword">year</span>, temperature
distribute <span class="org-keyword">by</span> <span class="org-keyword">year</span>
sort <span class="org-keyword">by</span> <span class="org-keyword">year</span> <span class="org-keyword">asc</span>, temperature <span class="org-keyword">desc</span>;
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org37dc70b" class="outline-3">
<h3 id="org37dc70b">调用外部脚本</h3>
<div class="outline-text-3" id="text-org37dc70b">
<p>
在 Hive 中我们可以调用外部的脚本
</p>
<div class="container">
<pre><code class="python"><span class="org-comment-delimiter">#</span><span class="org-comment">!/usr/bin/env python</span>
<span class="org-keyword">import</span> re
<span class="org-keyword">import</span> sys
<span class="org-keyword">for</span> line <span class="org-keyword">in</span> sys.stdin:
<span class="org-highlight-indentation"><span class="org-highlight-indentation"> </span></span>   <span class="org-rainbow-delimiters-depth-1">(</span>year, temp, q<span class="org-rainbow-delimiters-depth-1">)</span> = line.strip<span class="org-rainbow-delimiters-depth-1">()</span>.split<span class="org-rainbow-delimiters-depth-1">()</span>
    <span class="org-keyword">if</span> <span class="org-rainbow-delimiters-depth-1">(</span>temp != <span class="org-string">"9999"</span> <span class="org-keyword">and</span> re.match<span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"[01459]"</span>, q<span class="org-rainbow-delimiters-depth-2">)</span><span class="org-rainbow-delimiters-depth-1">)</span>:
<span class="org-highlight-indentation"> </span>       <span class="org-keyword">print</span> <span class="org-string">"%s\t%s"</span> % <span class="org-rainbow-delimiters-depth-1">(</span>year, temp<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>

<p>
然后在 Hive 中，
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">ADD</span> FILE /Users/tom/book-workspace/hadoop-book/ch17-hive/ src/main/python/is_good_quality.py;

<span class="org-keyword">from</span> records2
<span class="org-keyword">select</span> <span class="org-keyword">transform</span><span class="org-rainbow-delimiters-depth-1">(</span><span class="org-keyword">year</span>, temperature, quality<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-keyword">using</span> <span class="org-string">'is_good_quality.py'</span>
<span class="org-keyword">as</span> <span class="org-keyword">year</span>, temperature;
</code></pre>
</div>

<ol class="org-ol">
<li>首先要添加脚本</li>
<li>然后将 year temperature quality 传递给 python 脚本</li>
<li>然后 python 脚本将输出给 year temperature</li>
</ol>
</div>
</div>

<div id="outline-container-orgc663148" class="outline-3">
<h3 id="orgc663148">连接</h3>
<div class="outline-text-3" id="text-orgc663148">
<ol class="org-ol">
<li><p>
内连接
</p>

<div class="container">
<pre><code class="sql"><span class="org-keyword">select</span> * <span class="org-keyword">from</span> sales;
Joe 2
Hank 4
Ali 0
Eve 3
Hank 2
</code></pre>
</div>

<p>
以及表 things
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> * <span class="org-keyword">FROM</span> things;
2 Tie
4 Coat
3 Hat
1 Scarf
</code></pre>
</div>

<p>
下面我们使用内连接
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> sales.*, things.*
<span class="org-keyword">from</span> sales <span class="org-keyword">join</span> things <span class="org-keyword">on</span> <span class="org-rainbow-delimiters-depth-1">(</span>sales.id=things.id<span class="org-rainbow-delimiters-depth-1">)</span>;

Joe    2    2    Tie
Hank   4    4    Coat
Eve    3    3    Hat
Hank   2    2    Tie
</code></pre>
</div></li>

<li><p>
外连接首先看看左外连接
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> sales.*, things.*
<span class="org-keyword">FROM</span> sales <span class="org-keyword">LEFT</span> <span class="org-keyword">OUTER</span> <span class="org-keyword">JOIN</span> things <span class="org-keyword">ON</span> <span class="org-rainbow-delimiters-depth-1">(</span>sales.id = things.id<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>
<p>
如果出现两个表不能匹配的项，左边的表中的数据也可以列出来。
</p>

<p>
例如下面是输出的结果：
</p>
<div class="container">
<pre><code class="python">Joe    2    2    Tie
Hank   4    4    Coat
Ali    0    NULL NULL
Eve    3    3    Hat
Hank   2    2    Tie
</code></pre>
</div>

<p>
同理，还有右外连接，以及全连接
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> sales.*, things.*
<span class="org-keyword">from</span> sales.* <span class="org-keyword">right</span> <span class="org-keyword">outer</span> <span class="org-keyword">join</span> things <span class="org-keyword">on</span> <span class="org-rainbow-delimiters-depth-1">(</span>sales.id = things.id<span class="org-rainbow-delimiters-depth-1">)</span>;

<span class="org-keyword">select</span> * <span class="org-keyword">from</span> sales.*, things.*
<span class="org-keyword">from</span> sales <span class="org-keyword">full</span> <span class="org-keyword">outer</span> <span class="org-keyword">join</span> things <span class="org-keyword">on</span> <span class="org-rainbow-delimiters-depth-1">(</span>sales.id = things.id<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<div class="container">
<pre><code class="python">Joe    2    2    Tie
Hank   2    2    Tie
Hank   4    4    Coat
Eve    3    3    Hat
NULL   NULL 1    Scarf

<span class="org-comment-delimiter">#</span><span class="org-comment">----full outer join-----</span>
Ali    0    NULL NULL
NULL   NULL 1    Scarf
Hank   2    2    Tie
Joe    2    2    Tie
Eve    3    3    Hat
Hank   4    4    Coat
</code></pre>
</div></li>

<li><p>
半连接
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">select</span> * <span class="org-keyword">from</span> things
<span class="org-keyword">where</span> things.id <span class="org-keyword">in</span> <span class="org-rainbow-delimiters-depth-1">(</span><span class="org-keyword">select</span> id <span class="org-keyword">from</span> sales<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div></li>

<li>map 连接如果一个连接表小到足以放入到内存中，Hive 就可以把每个较小的表放入到 mapper 的内存中执行。</li>
</ol>
</div>
</div>

<div id="outline-container-org45f708b" class="outline-3">
<h3 id="org45f708b">子查询</h3>
<div class="outline-text-3" id="text-org45f708b">
<p>
Hive 对子查询的支持非常的有限。它只支持将子查询放入到 From 中。
</p>

<div class="container">
<pre><code class="sql"><span class="org-keyword">SELECT</span> station, <span class="org-keyword">year</span>, <span class="org-builtin">AVG</span><span class="org-rainbow-delimiters-depth-1">(</span>max_temperature<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-keyword">FROM</span> <span class="org-rainbow-delimiters-depth-1">(</span>
  <span class="org-keyword">SELECT</span> station, <span class="org-keyword">year</span>, <span class="org-builtin">MAX</span><span class="org-rainbow-delimiters-depth-2">(</span>temperature<span class="org-rainbow-delimiters-depth-2">)</span> <span class="org-keyword">AS</span> max_temperature
  <span class="org-keyword">FROM</span> records2
  <span class="org-keyword">WHERE</span> temperature != 9999 <span class="org-keyword">AND</span> quality <span class="org-keyword">IN</span> <span class="org-rainbow-delimiters-depth-2">(</span>0, 1, 4, 5, 9<span class="org-rainbow-delimiters-depth-2">)</span>
  <span class="org-keyword">GROUP</span> <span class="org-keyword">BY</span> station, <span class="org-keyword">year</span>
<span class="org-rainbow-delimiters-depth-1">)</span> mt
<span class="org-keyword">GROUP</span> <span class="org-keyword">BY</span> station, <span class="org-keyword">year</span>;
</code></pre>
</div>
<p>
其中 mt 是别名。
</p>
</div>
</div>


<div id="outline-container-orgb8f41d9" class="outline-3">
<h3 id="orgb8f41d9">视图</h3>
<div class="outline-text-3" id="text-orgb8f41d9">
<div class="container">
<pre><code class="sql"><span class="org-keyword">create</span> <span class="org-keyword">view</span> <span class="org-function-name">valid_records</span> <span class="org-keyword">as</span>
<span class="org-keyword">select</span> * <span class="org-keyword">from</span> records2
<span class="org-keyword">where</span> temperature != 9999 <span class="org-keyword">and</span> quality <span class="org-keyword">in</span> <span class="org-rainbow-delimiters-depth-1">(</span>0, 1, 4, 5, 9<span class="org-rainbow-delimiters-depth-1">)</span>;
</code></pre>
</div>

<p>
现在我们查询视图和表其实是一样的
</p>
<div class="container">
<pre><code class="sql"><span class="org-keyword">create</span> <span class="org-keyword">view</span> <span class="org-function-name">max_temperature</span> <span class="org-rainbow-delimiters-depth-1">(</span>station, <span class="org-keyword">year</span>, max_temperature<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-keyword">as</span>
<span class="org-keyword">select</span> station, <span class="org-keyword">year</span>, <span class="org-builtin">max</span><span class="org-rainbow-delimiters-depth-1">(</span>temperature<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-keyword">from</span> valid_records
<span class="org-keyword">group</span> <span class="org-keyword">by</span> station, <span class="org-keyword">year</span>;
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org2936ae9" class="outline-3">
<h3 id="org2936ae9">用户定义函数</h3>
<div class="outline-text-3" id="text-org2936ae9">
<p>
如果 Hive 内置的函数不能实现我们的需求时，我们可以通过写用户定义函数（UDF）来实现。
</p>

<p>
这个函数通过 Java 来写，然后在 Hive 中注册就可以使用了。
具体的用法看 P510
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<script type="text/javascript" src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/datejs/1.0/date.min.js"></script>

<script>
var base_url = 'https://api.github.com';
var title = document.title;
var owner = 'yydai';
var repo = 'yydai.github.io';
var search_issues = base_url + '/search/issues?q=' + title + '+user:' + owner + '+label:blog'+ '+state:open';

console.log("search_issues = "+ search_issues);

function test() {
  jQuery.ajax({
      type: 'GET',
      async: false,
      dataType:'json',
      url: search_issues,
      success:function(data) {
         result = data;
      }
  });
  return result;
}
var result = test();
var items = result.items[0];
if(jQuery.isEmptyObject(items)) {
    create(title);
} else {
    html_url = items.html_url;
	document.body.innerHTML +=
'<div id="comments"><h2>Comments</h2><div id="header">Want to leave a comment? Visit <a href="'+ html_url + '"> this issue page on GitHub</a> (you will need a GitHub account).</div></div>'
}


function create(title) {
	var create_url = 'https://blog-api-server.herokuapp.com/issues?title=' + title + '&labels=blog&body=Welcome to leave comments here.&owner=yydai&repo=yydai.github.io&auth=eXlkYWk6ZGVpc3Q5MjgxNw=='

	jQuery.ajax({
      type: 'GET',
      async: false,
      dataType:'json',
      url: create_url,
      success:function(data) {
         result = data;
      }
  });
}


console.log("total_count = " + result.total_count);
if(result.total_count == 1) {
    var comments_url = result.items[0].comments_url;
} else if (result.total_count == 0) {
        // create a new issue
    //create(title);
} else {
        // result not only
        alert('Cannot load the comments.');
}

function loadComments(data) {
	repo = 'github.com'
    for (var i=0; i<data.length; i++) {
      var cuser = data[i].user.login;
      var cuserlink = 'https://' + repo + '/' + data[i].user.login;
      var cbody = data[i].body_html;
      var cavatarlink = data[i].user.avatar_url;
      var cdate = Date.parse(data[i].created_at).toString('yyyy-MM-dd HH:mm:ss');

	  var html_url = items.html_url + '#issuecomment-' + data[i].url.substring(data[i].url.lastIndexOf('/')+1);

      var code = '<div class="comment"><div class="commentheader"><div class="commentgravatar">' + '<img src="' + cavatarlink + '" alt="" width="20" height="20">' + '</div><a class="commentuser" href="'+ cuserlink + '">' + cuser + '</a><a class="commentdate" href="' + html_url + '">' + cdate + '</a></div><div class="commentbody">' + cbody + '</div></div>';

      $('#comments').append(code);
    }
  }


var comments_api = comments_url + '?per_page=100';
console.log("comments api: " + comments_api);
$.ajax(comments_api, {
    headers: {Accept: 'application/vnd.github.full+json'},
    dataType: 'json',
    success: function(msg){
      loadComments(msg);
   }
  });


</script>

<hr />
 <div class='footer'>
© 2017 yydai<br/>
Email: dai92817@icloud.com
</div>
</div>
</body>
</html>
