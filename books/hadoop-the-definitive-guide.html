<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2017-09-23 Sat 11:49 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Hadoop, the Definitive Guide</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="ying dai" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<script src='https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js'></script>
<link href='http://apps.bdimg.com/libs/highlight.js/9.1.0/styles/zenburn.min.css' rel='stylesheet'>
<script src='http://apps.bdimg.com/libs/highlight.js/9.1.0/highlight.min.js'></script>
<script>hljs.initHighlightingOnLoad();</script>
<link rel='stylesheet' href='../css/worg2.css' typbe='text/css'/>
<link rel='shortcut icon' type='image/x-icon' href='/favicon.ico'>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2017 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="preamble" class="status">

<div class='nav'>
<div class='blog' style='text-align:right'>
<a href='/index.html'> Home </a> | <a href='/contact.html'> About </a>
</div>
</div>
</div>
<div id="content">
<h1 class="title">Hadoop, the Definitive Guide</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org6c19ba6">Chapter 1 初识 Hadoop</a>
<ul>
<li><a href="#org5c92b24">Hadoop 生态系统</a></li>
<li><a href="#orgc89f05b">兼容性</a></li>
</ul>
</li>
<li><a href="#orga032525">Chapter 2</a>
<ul>
<li><a href="#org5bd3715">气象数据集</a></li>
</ul>
</li>
<li><a href="#orgd53927a">Chapter 3 分布式文件系统</a>
<ul>
<li><a href="#orgbe1aa8e">HDFS 的设计</a></li>
<li><a href="#org75c0259">HDFS 的概念</a></li>
<li><a href="#org5032366">namenode 和 datanode</a></li>
<li><a href="#orga9f80c8">联邦 HDFS</a></li>
<li><a href="#orgf71b631">命令行接口</a></li>
<li><a href="#orgc41d98f">Java 接口</a></li>
<li><a href="#org1237707">数据流</a></li>
</ul>
</li>
</ul>
</div>
</div>
<hr />

<div id="outline-container-org6c19ba6" class="outline-2">
<h2 id="org6c19ba6">Chapter 1 初识 Hadoop</h2>
<div class="outline-text-2" id="text-org6c19ba6">
<p>
This is a long time to read all data on a single drive—and writing is even slower. The obvious way to reduce the time is to read from multiple disks at once. Imagine if we had 100 drives, each holding one hundredth of the data. Working in parallel, we could read the data in under two minutes.
</p>

<p>
The first problem to solve is hardware failure: as soon as you
start using many pieces of hardware, the chance that one will fail
is fairly high. A common way of avoiding data loss is through
replication: redundant copies of the data are kept by the system
so that in the event of failure, there is another copy available.
</p>

<p>
总结来说，Hadoop 对数据的存储和分析提供了一种可靠的、可扩展的的平台。
</p>

<p>
为什么不能用关系型数据库来对大量硬盘上的数据进行批量分析呢？
如果数据访问模式包含大量的硬盘寻址，那么读取大量的数据集就会很慢。另一方面，如果数据库只是更新一小部分的内容，传统的 B 树更加有优势。
</p>

<p>
MapReduce 适合一次写入、多次读取的应用，关系数据库更适持续更新的数据集。
</p>


<div class="figure">
<p><img src="./imgs/20170921_120427_77207zxU.png" alt="20170921_120427_77207zxU.png" width="100%" />
</p>
</div>


<p>
MapReduce 对于非结构化数据或半结构化数据非常有效。
MapReduce 尽量在计算节点上存储数据，以实现数据的本地快速访问。数据本地化是它的核心特征。
</p>
</div>


<div id="outline-container-org5c92b24" class="outline-3">
<h3 id="org5c92b24">Hadoop 生态系统</h3>
<div class="outline-text-3" id="text-org5c92b24">
<p>
Hadoop 因 MapReduce 和 HDFS 而出名，但是它也泛指一系列相关的项目。
</p>
</div>
</div>

<div id="outline-container-orgc89f05b" class="outline-3">
<h3 id="orgc89f05b">兼容性</h3>
<div class="outline-text-3" id="text-orgc89f05b">
<p>
Hadoop 升级要考虑到兼容性：API 兼容性、数据兼容性和连接兼容性。
</p>
</div>
</div>
</div>

<div id="outline-container-orga032525" class="outline-2">
<h2 id="orga032525">Chapter 2</h2>
<div class="outline-text-2" id="text-orga032525">
</div><div id="outline-container-org5bd3715" class="outline-3">
<h3 id="org5bd3715">气象数据集</h3>
<div class="outline-text-3" id="text-org5bd3715">
<p>
这个例子中，要写一个挖掘气象数据的程序。
</p>
</div>

<ul class="org-ul"><li><a id="org9e7ab56"></a>使用 awk 处理<br /><div class="outline-text-4" id="text-org9e7ab56">
<p>
首先看下使用 awk 来处理 <a href="https://github.com/learning-from-books/hadoop-book/blob/master/ch02-mr-intro/src/main/awk/max_temperature.sh">code</a>
</p>
</div></li>

<li><a id="org900b725"></a>Hadoop 来处理<br /><div class="outline-text-4" id="text-org900b725">
<p>
map 处理输入的数据，得到 （年，温度）的键值对。
</p>

<p>
reduce 接收到 (年，[温度列表])， 然后找到最大温度。
</p>



<div class="figure">
<p><img src="./imgs/20170921_155020_77207aQn.png" alt="20170921_155020_77207aQn.png" width="100%" />
</p>
</div>


<p>
这部分的代码见 github.
</p>
</div></li>

<li><a id="org0ae82b5"></a>横向扩展<br /><div class="outline-text-4" id="text-org0ae82b5">
<p>
为了实现横向扩展，我们需要把数据存储在分布式文件系统中，一般为 HDFS。
</p>
</div></li>

<li><a id="org12866d5"></a>数据流<br /><div class="outline-text-4" id="text-org12866d5">
<p>
MapReduce 作业是客户端需要执行的一个工作单元：它包括输入数据、MapReduce 程序和配置信息。Hadoop 将作业粉刺若干个小任务来执行：map 任务和 reduce 任务。
</p>

<p>
Hadoop 将 MapReduce 的输入数据划分成等长的小数据块，称为“分片”。每个分片构建一个
map 任务。 一个合理的分片大小为 HDFS 的一个块的大小，默认 64M
</p>

<p>
Hadoop 在存储有输入数据的节点是运行 map 可以获得最佳性能。这就是所谓的“数据本地优化”。map 任务将其输出写入到本地硬盘，而非 HDFS。这是因为它的结果是中间结果，reduce 处理后才是最终结果。
所以，中间结果存到 hdfs 意义不大。
</p>

<p>
reduce 任务不具有数据本地化的优势。下面是一个 reduce 任务的 MapReduce 数据流
</p>


<div class="figure">
<p><img src="./imgs/20170921_171719_77207nat.png" alt="20170921_171719_77207nat.png" width="100%" />
</p>
</div>
</div></li>

<li><a id="orgee30f21"></a>combiner 函数<br /><div class="outline-text-4" id="text-orgee30f21">
<p>
Hadoop 允许用户针对 map 任务的输出指定一个 combiner。
combiner 的输出作为 reduce 函数的输入。combiner 属于一种优化方案。
</p>

<p>
以上面温度的例子来说明。
第一个 map：
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, 0<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 20<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 10<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>
<p>
第二个 map:
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, 25<span class="org-rainbow-delimiters-depth-1">)</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 15<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>

<p>
reduce 函数调用时输入:
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, <span class="org-rainbow-delimiters-depth-2">[</span>0, 20, 10, 25, 15<span class="org-rainbow-delimiters-depth-2">]</span><span class="org-rainbow-delimiters-depth-1">)</span>

<span class="org-comment-delimiter"># </span><span class="org-comment">&#36755;&#20986;</span>
<span class="org-rainbow-delimiters-depth-1">(</span>1950, 25<span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>

<p>
但是我们如果用了 combiner 以后，我们就可以提前找到每个 map 任务的输出结果的最高气温。现在 reduce 的输入变成了。
</p>
<div class="container">
<pre><code class="python"><span class="org-rainbow-delimiters-depth-1">(</span>1950, <span class="org-rainbow-delimiters-depth-2">[</span>20, 25<span class="org-rainbow-delimiters-depth-2">]</span><span class="org-rainbow-delimiters-depth-1">)</span>
</code></pre>
</div>
</div></li>

<li><a id="org310a31a"></a>Hadoop Streaming<br /><div class="outline-text-4" id="text-org310a31a">
<p>
它提供了 MapReduce 的 API，我们可以用其他的语言来写 map 和 reduce。
</p>
</div></li></ul>
</div>
</div>


<div id="outline-container-orgd53927a" class="outline-2">
<h2 id="orgd53927a">Chapter 3 分布式文件系统</h2>
<div class="outline-text-2" id="text-orgd53927a">
<p>
当数据集的大小超过一台独立的物理计算机的存储能力时，有必要对它分区并存储到若干的计算机上，也就需要一个分布式文件系统。
</p>

<p>
这部分主要介绍 HDFS 文件系统。
</p>
</div>


<div id="outline-container-orgbe1aa8e" class="outline-3">
<h3 id="orgbe1aa8e">HDFS 的设计</h3>
<div class="outline-text-3" id="text-orgbe1aa8e">
<p>
HDFS 流式数据访问模式来存储超大文件。
</p>
<ul class="org-ul">
<li>超大文件</li>
<li><p>
流式数据访问。
</p>

<p>
一次写入，多次读取时最高效的访问模式。
</p></li>
<li><p>
商用硬件
</p>

<p>
Hadoop 并不需要运行在昂贵且可靠的硬件上。
</p></li>
<li><p>
低延迟的数据访问
</p>

<p>
要求低时间的应用不适合 HDFS, 它是为高吞吐量应用优化的。
</p></li>
<li><p>
大量的小文件
</p>

<p>
namenode 将文件系统的元数据存储在内存中，因此文件系统的文件总数受限于 namenode 的内存容量。
每个文件、目录和数据块的存储信息约占 150B。
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org75c0259" class="outline-3">
<h3 id="org75c0259">HDFS 的概念</h3>
<div class="outline-text-3" id="text-org75c0259">
<p>
HDFS 的块要比磁盘默认的块大很多，如果一个小于块大小的的文件不会占据整个的块空间。
</p>

<p>
为什么 HDFS 块这么大？原因是为了最小化寻址开销。例如，如果寻址时间为 10ms，而传输速率为 100MB/s，为了使寻址时间仅占用 1%，我们要将块的大小设置为大约 100MB。
</p>

<p>
块非常适合数据备份进而提供数据容错能力和提高可用性。将每个块复制到少数几个独立的机器上，可以确保块、磁盘或者机器发生故障后数据的恢复。
</p>

<p>
HDFS 中的 fsck 指令可以显示块信息：
</p>
<div class="container">
<pre><code class="shell">hadoop fsck / -files -blocks
</code></pre>
</div>
</div>
</div>

<div id="outline-container-org5032366" class="outline-3">
<h3 id="org5032366">namenode 和 datanode</h3>
<div class="outline-text-3" id="text-org5032366">
<p>
HDFS 有两类节点以管理者-工作者模式运行，一个是 namenode，一个 datanode.
namenode 维护文件系统树以及所有的文件和目录。他们通过：命名空间镜像文件和编辑日志文件来永久的保存在本地的本地磁盘上。
</p>

<p>
客户端(client) 代表用户通过与 namenode 和 datanode 交互来访问整个文件系统。
datanode 是系统的工作节点，它根据需要存储并检索数据块，并定期向 namenode 发送他们所存储的块的列表。
如果 namenode 机器毁掉，我们的文件将会丢失，因为我们无法重建我们的数据。Hadoop 提供了两种方式：
</p>
<ul class="org-ul">
<li>备份元数据持久状态的文件</li>
<li>运行一个辅助的 namenode, 但它不能被用作 namenode。</li>
</ul>
</div>
</div>
<div id="outline-container-orga9f80c8" class="outline-3">
<h3 id="orga9f80c8">联邦 HDFS</h3>
<div class="outline-text-3" id="text-orga9f80c8">
<p>
在联邦环境下，有多个的 namenode，它们管理不同的命名空间，它们之间互相不影响。
</p>
</div>
</div>
<div id="outline-container-orgf71b631" class="outline-3">
<h3 id="orgf71b631">命令行接口</h3>
<div class="outline-text-3" id="text-orgf71b631">
<p>
在配置中，我们需要设置 fs.default.name，例如 hdfs://localhost/, 用于设置 Hadoop 的默认文件系统。
第二个属性，dfs.replication, 设置为 1，这样它就不会将文件系统块复制 3 份了。
</p>

<p>
文件系统的基本操作
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt hdfs://localhost/user/quangle.txt
</code></pre>
</div>

<p>
我们其实可以简化这个操作
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt /user/quangle.txt
</code></pre>
</div>

<p>
也可以使用相对路径, 复制到 HOMΕ 中
</p>
<div class="container">
<pre><code class="shell">hadoop fs -copyFromLocal input/docs/quangle.txt quangle.txt
</code></pre>
</div>

<p>
还有
</p>
<div class="container">
<pre><code class="shell">hadoop fs -mkdir ...
hadoop fs -copyToLocal ...
</code></pre>
</div>
</div>
</div>
<div id="outline-container-orgc41d98f" class="outline-3">
<h3 id="orgc41d98f">Java 接口</h3>
<div class="outline-text-3" id="text-orgc41d98f">
<p>
从 Hadoop URL 读取数据
</p>
<div class="container">
<pre><code class="java"><span class="org-type">InputStream</span> <span class="org-variable-name">in</span> = <span class="org-constant">null</span>;
<span class="org-keyword">try</span> <span class="org-rainbow-delimiters-depth-1">{</span>
    in = <span class="org-keyword">new</span> <span class="org-type">URL</span><span class="org-rainbow-delimiters-depth-2">(</span><span class="org-string">"hdfs://host/path"</span><span class="org-rainbow-delimiters-depth-2">)</span>.openStream<span class="org-rainbow-delimiters-depth-2">()</span>;
    <span class="org-comment-delimiter">// </span><span class="org-comment">process in</span>
<span class="org-rainbow-delimiters-depth-1">}</span> <span class="org-keyword">finally</span> <span class="org-rainbow-delimiters-depth-1">{</span>
    IOUtils.closeStream<span class="org-rainbow-delimiters-depth-2">(</span>in<span class="org-rainbow-delimiters-depth-2">)</span>;
<span class="org-rainbow-delimiters-depth-1">}</span>
</code></pre>
</div>

<p>
通过 URLStreamHandler 实例以标准输出方式显示 Hadoop 文件
</p>

<div class="container">
<pre><code class="java"><span class="org-keyword">public</span> <span class="org-keyword">class</span> <span class="org-type">URLCat</span><span class="org-rainbow-delimiters-depth-1">{</span>
    <span class="org-keyword">static</span> <span class="org-rainbow-delimiters-depth-2">{</span>
        URL.setURLStreamHandlerFactory<span class="org-rainbow-delimiters-depth-3">(</span><span class="org-keyword">new</span> <span class="org-type">FsUrlStreamHandlerFactory</span><span class="org-rainbow-delimiters-depth-4">()</span><span class="org-rainbow-delimiters-depth-3">)</span>;
    <span class="org-rainbow-delimiters-depth-2">}</span>
    <span class="org-keyword">public</span> <span class="org-keyword">static</span> <span class="org-type">void</span> <span class="org-function-name">main</span><span class="org-rainbow-delimiters-depth-2">(</span><span class="org-type">String</span><span class="org-rainbow-delimiters-depth-3">[]</span> <span class="org-variable-name">args</span><span class="org-rainbow-delimiters-depth-2">)</span> <span class="org-keyword">throws</span> <span class="org-type">Exception</span> <span class="org-rainbow-delimiters-depth-2">{</span>
        <span class="org-type">InputStream</span> <span class="org-variable-name">in</span> = <span class="org-constant">null</span>;
        <span class="org-keyword">try</span> <span class="org-rainbow-delimiters-depth-3">{</span>
            in = <span class="org-keyword">new</span> <span class="org-type">URL</span><span class="org-rainbow-delimiters-depth-4">(</span>args<span class="org-rainbow-delimiters-depth-5">[</span>0<span class="org-rainbow-delimiters-depth-5">]</span>.openStream<span class="org-rainbow-delimiters-depth-5">()</span><span class="org-rainbow-delimiters-depth-4">)</span>;
            IOUtils.copyBytes<span class="org-rainbow-delimiters-depth-4">(</span>in, System.out, 4096, <span class="org-constant">false</span><span class="org-rainbow-delimiters-depth-4">)</span>;
        <span class="org-rainbow-delimiters-depth-3">}</span> <span class="org-keyword">finally</span> <span class="org-rainbow-delimiters-depth-3">{</span>
            IOUtils.closeStream<span class="org-rainbow-delimiters-depth-4">(</span>in<span class="org-rainbow-delimiters-depth-4">)</span>;
        <span class="org-rainbow-delimiters-depth-3">}</span>
    <span class="org-rainbow-delimiters-depth-2">}</span>
<span class="org-rainbow-delimiters-depth-1">}</span>
</code></pre>
</div>

<p>
我们可这样运行
</p>
<div class="container">
<pre><code class="shell">hadoop URLCat hdfs://localhost/user/tom/quangle.txt
</code></pre>
</div>
</div>
</div>
<div id="outline-container-org1237707" class="outline-3">
<h3 id="org1237707">数据流</h3>
<div class="outline-text-3" id="text-org1237707">
<p>
下面先看文件的读取。
为了了解客户端以及与之交互的 HDFS、namenode 和 datanode
之间的数据流是什么样的，我们看下图。
</p>

<div class="figure">
<p><img src="./imgs/20170922_210837_77207z4I.png" alt="20170922_210837_77207z4I.png" width="100%" />
</p>
</div>

<p>
客户端通过调用 FileSyste 对象的 open() 方法来打开要读取的文件，
对于 HDFS 来说，这个对象是分布式文件系统的一个实例。分布式文件系统通过
RPC 来调用 namenode，以确定文件块的起始位置。对于每一个块，namenode 返回存有该块副本的 datanode 地址。客户端读取数据，块是按照打开 DFSInputStream 与 datanode
新建连接的顺序读取的。
</p>

<p>
如果 DFSInputStream 在于 datanode 通信时遇到错误，它会尝试从这个快的另外一个最邻近的 datanode 读取数据。
它也记住那个故障 datanode，以保证以后不会反复读取该节点上后续的块。这个设计的一个重点是，namenode 告知客户端每个块的最佳 datanode，并让客户端直接连接到 datanode 检索数据。这里有个节点“彼此近邻”的问题？如何定义它？
一个简单的方式：把网络看做一个树，两个节点间的距离是它们到最近共同祖先的距离和。
</p>


<p>
下面看下文件的写入。
</p>

<p>
我们要考虑的是如何新建一个文件，把数据写入到文件，最后关闭该文件。
</p>

<div class="figure">
<p><img src="./imgs/20170922_214041_77207ADP.png" alt="20170922_214041_77207ADP.png" width="100%" />
</p>
</div>

<p>
首先 namenode 会执行各种的检查来确保这个文件不存在以及客户端有新建文件的权限。
DitributedFileSytem 向客户端返回一个 FSDataOutputStream 对象，客户端开始写入数据。
注意其中的 4、5 的过程，其中三个 datanode 是节点的副本，默认为 3 个。
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<script type="text/javascript" src="https://cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/datejs/1.0/date.min.js"></script>

<script>
var base_url = 'https://api.github.com';
var title = document.title;
var owner = 'yydai';
var repo = 'yydai.github.io';
var search_issues = base_url + '/search/issues?q=' + title + '+user:' + owner + '+label:blog'+ '+state:open';

console.log("search_issues = "+ search_issues);

function test() {
  jQuery.ajax({
      type: 'GET',
      async: false,
      dataType:'json',
      url: search_issues,
      success:function(data) {
         result = data;
      }
  });
  return result;
}
var result = test();
var items = result.items[0];
if(jQuery.isEmptyObject(items)) {
    create(title);
} else {
    html_url = items.html_url;
	document.body.innerHTML +=
'<div id="comments"><h2>Comments</h2><div id="header">Want to leave a comment? Visit <a href="'+ html_url + '"> this issue page on GitHub</a> (you will need a GitHub account).</div></div>'
}


function create(title) {
	var create_url = 'https://blog-api-server.herokuapp.com/issues?title=' + title + '&labels=blog&body=Welcome to leave comments here.&owner=yydai&repo=yydai.github.io&auth=eXlkYWk6ZGVpc3Q5MjgxNw=='

	jQuery.ajax({
      type: 'GET',
      async: false,
      dataType:'json',
      url: create_url,
      success:function(data) {
         result = data;
      }
  });
}


console.log("total_count = " + result.total_count);
if(result.total_count == 1) {
    var comments_url = result.items[0].comments_url;
} else if (result.total_count == 0) {
        // create a new issue
    //create(title);
} else {
        // result not only
        alert('Cannot load the comments.');
}

function loadComments(data) {
	repo = 'github.com'
    for (var i=0; i<data.length; i++) {
      var cuser = data[i].user.login;
      var cuserlink = 'https://' + repo + '/' + data[i].user.login;
      var cbody = data[i].body_html;
      var cavatarlink = data[i].user.avatar_url;
      var cdate = Date.parse(data[i].created_at).toString('yyyy-MM-dd HH:mm:ss');

	  var html_url = items.html_url + '#issuecomment-' + data[i].url.substring(data[i].url.lastIndexOf('/')+1);

      var code = '<div class="comment"><div class="commentheader"><div class="commentgravatar">' + '<img src="' + cavatarlink + '" alt="" width="20" height="20">' + '</div><a class="commentuser" href="'+ cuserlink + '">' + cuser + '</a><a class="commentdate" href="' + html_url + '">' + cdate + '</a></div><div class="commentbody">' + cbody + '</div></div>';

      $('#comments').append(code);
    }
  }


var comments_api = comments_url + '?per_page=100';
console.log("comments api: " + comments_api);
$.ajax(comments_api, {
    headers: {Accept: 'application/vnd.github.full+json'},
    dataType: 'json',
    success: function(msg){
      loadComments(msg);
   }
  });


</script>

<hr />
 <div class='footer'>
© 2017 yydai<br/>
Email: dai92817@icloud.com
</div>
</div>
</body>
</html>
